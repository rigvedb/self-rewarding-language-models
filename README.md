# self-rewarding-language-models
This repositoy is dedicated to experimentation and implication of a training framework proposed by META in the research paper Self_rewarding Language models


**INITIALIZATION**


We are gonna be using three types of fine-tuning:-
SFT :- supervised fine tuning
IFT:- Instruction Fine tuning
EFT :- Evaluation fine tuning

chosen format for LLM-as-a-judge instruction following data :-
![LLM as a judge](https://github.com/rigvedb/self-rewarding-language-models/assets/122348418/9eea8cee-30eb-4feb-93f9-b8357ad2495f)


